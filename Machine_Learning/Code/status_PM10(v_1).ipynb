{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "status_PM10(v.1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg0tDTFi6UHE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "metadata": {
        "id": "5IUiMUYS6rlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "QytFlbAf6t-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "GO8OR2QZ6wS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download image zip\n",
        "downloaded = drive.CreateFile({'id':\"10K44l5A__7DrNTZStVP6-vebBbWp9f4Y\"})\n",
        "downloaded.GetContentFile('Dataset.zip') \n",
        "\n",
        "# Download CSV data\n",
        "downloaded = drive.CreateFile({'id':\"1v73UDBS2_yUMZ0yTAIrN8i5JyiepIAKo\"})\n",
        "downloaded.GetContentFile('PM10_Data.csv') "
      ],
      "metadata": {
        "id": "UPievaOi6waQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_local_zip = \"/content/Dataset.zip\"\n",
        "\n",
        "zip_ref = zipfile.ZipFile(test_local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/Dataset')\n",
        "\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "XAF81oRd65iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training and validation base directories\n",
        "train_dir = '/tmp/Dataset'\n",
        "\n",
        "# Directory with training Good pictures\n",
        "train_Good_dir = os.path.join(train_dir, 'Good')\n",
        "# Directory with training Moderate pictures\n",
        "train_Moderate_dir = os.path.join(train_dir, 'Moderate')\n",
        "# Directory with training Dangerous pictures\n",
        "train_Dangerous_dir = os.path.join(train_dir, 'Dangerous')\n",
        "\n",
        "# Check the number of images for each class and set\n",
        "print(f\"There are {len(os.listdir(train_Good_dir))} images of Good.\\n\")\n",
        "print(f\"There are {len(os.listdir(train_Moderate_dir))} images of Moderate.\\n\")\n",
        "print(f\"There are {len(os.listdir(train_Dangerous_dir))} images of Dangerous.\\n\")"
      ],
      "metadata": {
        "id": "O9uaEuTg6-oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn all image to jpg\n",
        "turn_to_jpg(train_Good_dir)\n",
        "turn_to_jpg(train_Moderate_dir)\n",
        "turn_to_jpg(train_Dangerous_dir)"
      ],
      "metadata": {
        "id": "VfiZef7K7HCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the first example of a Good\n",
        "sample_image  = load_img(f\"{os.path.join(train_Good_dir, os.listdir(train_Good_dir)[0])}\")\n",
        "\n",
        "# Convert the image into its numpy array representation\n",
        "sample_array = img_to_array(sample_image)\n",
        "\n",
        "print(f\"Each image has shape: {sample_array.shape}\")"
      ],
      "metadata": {
        "id": "WMqpu4R57Hz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV File\n",
        "csv_path = \"/content/PM10_Data.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "df.head(30)"
      ],
      "metadata": {
        "id": "i24n3dGC7NXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_missing_image(dataset, image_path):\n",
        "    dataset = np.array(dataset)\n",
        "    result = []\n",
        "    data_index = 0\n",
        "    for i in dataset:\n",
        "      if os.path.join(i[0] + \".jpg\") not in os.listdir(os.path.join(image_path, i[2])):\n",
        "        result.append(data_index)\n",
        "      data_index += 1\n",
        "    return result"
      ],
      "metadata": {
        "id": "PvAkCoH97P8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data with ratio train 90% and validation 10%\n",
        "randomize_train = df.sample(frac = 0.9)\n",
        "randomize_val = df.drop(randomize_train.index)\n",
        "not_found_image = remove_missing_image(randomize_train, train_dir)\n",
        "train_image = np.delete(np.array(randomize_train), not_found_image, axis=0)\n",
        "\n",
        "not_found_image = remove_missing_image(randomize_val, train_dir)\n",
        "val_image = np.delete(np.array(randomize_val), not_found_image, axis=0)"
      ],
      "metadata": {
        "id": "SUY7qtw37Sr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image_path, image_list):\n",
        "    ori_image = Image.open(os.path.join(image_path, image_list[2] + \"/\" + image_list[0] + \".jpg\"))\n",
        "    ori_image = ori_image.resize((150, 150))\n",
        "    gray_image = ImageOps.grayscale(ori_image)\n",
        "    return np.array(gray_image).flatten()"
      ],
      "metadata": {
        "id": "GnVPqQWv7VQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open image and put it in array\n",
        "image_array = []\n",
        "for i in train_image:\n",
        "  image_array.append(process_image(train_dir, i))\n",
        "image_array = np.array(image_array)\n",
        "\n",
        "val_array = []\n",
        "for i in val_image:\n",
        "  val_array.append(process_image(train_dir, i))\n",
        "val_array = np.array(val_array)"
      ],
      "metadata": {
        "id": "PSBZGg7z7YDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.delete(train_image, 0, 1)\n",
        "train_labels = np.delete(train_labels, 1, 1)\n",
        "\n",
        "val_labels = np.delete(val_image, 0, 1)\n",
        "val_labels = np.delete(val_labels, 1, 1)"
      ],
      "metadata": {
        "id": "us1skAhi7aa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_array = np.reshape(image_array, (image_array.shape[0], image_array.shape[1], image_array.shape[2], 1))\n",
        "val_array = np.reshape(val_array, (val_array.shape[0], val_array.shape[1], val_array.shape[2], 1))"
      ],
      "metadata": {
        "id": "t0_x9sCD7d_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_array = image_array/255.0\n",
        "val_array = val_array/255.0"
      ],
      "metadata": {
        "id": "XS2GZXvn7ekY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_labels.astype(float)\n",
        "val_labels = val_labels.astype(float)"
      ],
      "metadata": {
        "id": "MjflGywJ7erP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_array.shape"
      ],
      "metadata": {
        "id": "4U5eV1Vr7kC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.reshape(train_labels, (image_array.shape[0], 1))\n",
        "val_labels = np.reshape(val_labels, (val_array.shape[0], 1))"
      ],
      "metadata": {
        "id": "_E_SCI9L7o8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "id": "96baqA9G7pwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generators\n",
        "train_generator, validation_generator = train_labels, val_labels"
      ],
      "metadata": {
        "id": "AudXtqDS7vEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer Learning"
      ],
      "metadata": {
        "id": "AkltXHhf7zI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the inception v3 weights\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "metadata": {
        "id": "Mpi4OwMk7wrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the inception model  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Create an instance of the inception model from the local pre-trained weights\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "metadata": {
        "id": "wFvsyMsT73h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pre_trained_model(local_weights_file):\n",
        "  pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n",
        "                                  include_top = False, \n",
        "                                  weights = None) \n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  # Make non trainable layer\n",
        "  for layers in pre_trained_model.layers:\n",
        "    layers.trainable = False\n",
        "\n",
        "  return pre_trained_model"
      ],
      "metadata": {
        "id": "ilt3C56t74Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model = create_pre_trained_model(local_weights_file)\n",
        "\n",
        "# model summary\n",
        "pre_trained_model.summary()"
      ],
      "metadata": {
        "id": "Q7sG8hbk78Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.999):\n",
        "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True"
      ],
      "metadata": {
        "id": "W9N2B6RB7-MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_of_last_layer(pre_trained_model):\n",
        "  last_desired_layer = pre_trained_model.get_layer('mixed7')\n",
        "  print('last layer output shape: ', last_desired_layer.output_shape)\n",
        "  last_output = last_desired_layer.output\n",
        "  print('last layer output: ', last_output)\n",
        "\n",
        "  return last_output"
      ],
      "metadata": {
        "id": "JpFvJZl27-UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_output = output_of_last_layer(pre_trained_model)"
      ],
      "metadata": {
        "id": "kG2tfST28MA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the type of the pre-trained model\n",
        "print(f\"The pretrained model has type: {type(pre_trained_model)}\")"
      ],
      "metadata": {
        "id": "gIMZT8F48ONA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_final_model(pre_trained_model, last_output):\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  x = layers.Flatten()(last_output)\n",
        "\n",
        "  # Add a fully connected layer with 1024 hidden units and ReLU activation\n",
        "  x = layers.Dense(1024, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.2)(x)  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense(1, activation='sigmoid')(x)        \n",
        "\n",
        "  # Create the complete model by using the Model class\n",
        "  model = Model(inputs=pre_trained_model.input, outputs=x)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer = RMSprop(learning_rate=0.0001), \n",
        "                loss = 'binary_crossentropy',\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "1fXKl9eh8O26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model in a variable\n",
        "model = create_final_model(pre_trained_model, last_output)\n",
        "\n",
        "# Inspect parameters\n",
        "total_params = model.count_params()\n",
        "num_trainable_params = sum([w.shape.num_elements() for w in model.trainable_weights])\n",
        "\n",
        "print(f\"There are {total_params:,} total parameters in this model.\")\n",
        "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")"
      ],
      "metadata": {
        "id": "F-iAKMVj8XGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = myCallback()\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    epochs = 100,\n",
        "                    verbose = 2,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "Fzp7X4wB8X7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}