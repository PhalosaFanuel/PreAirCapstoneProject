{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd83d3-eaa9-4895-934f-e05632766b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting use LSTM For AQI (AIR QUALITY INDEX) Based on PM10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d985003d-9bb1-4c4d-b41f-cd61d0f3da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4da457a-5b0b-46b7-9f9c-19603a33ec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape = (1076, 10)\n",
      "All timestamp = 1076\n",
      "Columns Selected : ['pm10', 'so2', 'co']\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "dataset = pd.read_csv('ispu_data.csv')\n",
    "\n",
    "# Select column\n",
    "cols = list(dataset)[1:4]\n",
    "\n",
    "# Extracting\n",
    "datelist = list(dataset['tanggal'])\n",
    "datelist = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in datelist]\n",
    "\n",
    "print('Training set shape = {}'.format(dataset.shape))\n",
    "print('All timestamp = {}'.format(len(datelist)))\n",
    "print('Columns Selected : {}'.format(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e53405b8-bc7f-4d9e-98ba-1b662f683d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set = (1076, 3).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[29., 15.,  7.],\n",
       "       [24., 17.,  6.],\n",
       "       [23., 16.,  6.],\n",
       "       ...,\n",
       "       [61., 54., 15.],\n",
       "       [60., 53., 17.],\n",
       "       [64., 52., 44.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "dataset = dataset[cols].astype(str)\n",
    "for i in cols:\n",
    "    for j in range(0, len(dataset)):\n",
    "        dataset[i][j] = dataset[i][j].replace(',', '')\n",
    "\n",
    "dataset = dataset.astype(float)\n",
    "# Using feature columns\n",
    "training_set = dataset.values\n",
    "\n",
    "print('Shape of training set = {}.'.format(training_set.shape))\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5462839-0c75-4241-9077-0c4c9efbf839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.1921057 ],\n",
       "       [-2.52555766],\n",
       "       [-2.59224805],\n",
       "       ...,\n",
       "       [-0.0580132 ],\n",
       "       [-0.12470359],\n",
       "       [ 0.14205797]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "training_set_scaled = scaler.fit_transform(training_set)\n",
    "\n",
    "scaler_predict = StandardScaler()\n",
    "scaler_predict.fit_transform(training_set[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb6f3b2d-02c8-4e5a-9f3c-4a625780e57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape = (987, 60, 2).\n",
      "y_train shape = (987, 1).\n"
     ]
    }
   ],
   "source": [
    "# Create data structure 60 timestamps and 1 output\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "n_future = 30\n",
    "n_past = 60\n",
    "\n",
    "for i in range(n_past, len(training_set_scaled) - n_future + 1):\n",
    "    x_train.append(training_set_scaled[i - n_past:i, 0:dataset.shape[1] - 1])\n",
    "    y_train.append(training_set_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "print('x_train shape = {}.'.format(x_train.shape))\n",
    "print('y_train shape = {}.'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c427bb9a-67a2-4ff6-9416-b75f2de0daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "model = Sequential()\n",
    "# 1st LSTM layer\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(n_past, dataset.shape[1]-1)))\n",
    "# 2nd LSTM layer\n",
    "model.add(LSTM(units=10, return_sequences=False))\n",
    "# Dropout\n",
    "model.add(Dropout(0.25))\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "model.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cdfa76c-14c2-4b93-b034-8352e13f22bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 64)            17152     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10)                3000      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,163\n",
      "Trainable params: 20,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1970b542-320a-4ec2-be1a-eab73f1998a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9705\n",
      "Epoch 1: val_loss improved from inf to 1.09016, saving model to weights.h5\n",
      "4/4 [==============================] - 39s 4s/step - loss: 0.9705 - val_loss: 1.0902 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8096\n",
      "Epoch 2: val_loss did not improve from 1.09016\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.8096 - val_loss: 1.1078 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7507\n",
      "Epoch 3: val_loss did not improve from 1.09016\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.7507 - val_loss: 1.5243 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8513\n",
      "Epoch 4: val_loss did not improve from 1.09016\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.8513 - val_loss: 1.2005 - lr: 0.0100\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7520\n",
      "Epoch 5: val_loss improved from 1.09016 to 1.07002, saving model to weights.h5\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.7520 - val_loss: 1.0700 - lr: 0.0100\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7614\n",
      "Epoch 6: val_loss improved from 1.07002 to 1.06747, saving model to weights.h5\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.7614 - val_loss: 1.0675 - lr: 0.0100\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7432\n",
      "Epoch 7: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.7432 - val_loss: 1.2128 - lr: 0.0100\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7403\n",
      "Epoch 8: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.7403 - val_loss: 1.2764 - lr: 0.0100\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6870\n",
      "Epoch 9: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6870 - val_loss: 1.1860 - lr: 0.0100\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7121\n",
      "Epoch 10: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.7121 - val_loss: 1.3598 - lr: 0.0100\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7453\n",
      "Epoch 11: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.7453 - val_loss: 1.2583 - lr: 0.0100\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7136\n",
      "Epoch 12: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.7136 - val_loss: 1.1045 - lr: 0.0100\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6655\n",
      "Epoch 13: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6655 - val_loss: 1.1212 - lr: 0.0100\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6527\n",
      "Epoch 14: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6527 - val_loss: 1.1614 - lr: 0.0100\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6725\n",
      "Epoch 15: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6725 - val_loss: 1.1344 - lr: 0.0100\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6653\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.06747\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6653 - val_loss: 1.0802 - lr: 0.0100\n",
      "Epoch 16: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Training Model\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(x_train, y_train, shuffle=True, epochs=30, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5b82f2b-b670-46bb-84b9-3ae237ecc28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future Prediction\n",
    "datelist_future = pd.date_range(datelist[-1], periods=n_future, freq='1d').tolist()\n",
    "datelist_future_ = []\n",
    "for present_timestamp in datelist_future:\n",
    "    datelist_future_.append(present_timestamp.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29590b15-61d8-4020-b2bf-c96b564e5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions_future = model.predict(x_train[-n_future:])\n",
    "predictions_train = model.predict(x_train[n_past:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
